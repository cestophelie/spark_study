{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c17a6c5b-e6fd-4e66-8e0e-1895ef2ae6e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from  pyspark.sql.functions import concat_ws\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e5477af-5fd3-4810-9018-e9c7639333d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\nDataFrame[_c0: string]\n['tom 70', 'sara 80', 'joon 100', 'kevin 90', 'John 90']\n['70', '80', '100', '90', '90']\ndefaultdict(<class 'int'>, {'70': 1, '80': 1, '100': 1, '90': 2})\n90: 2\n70: 1\n80: 1\n100: 1\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import pyspark\n",
    "\n",
    "df = spark.table(\"default.grade_2_txt\")\n",
    "\n",
    "'''df_text = df.select(concat_ws(\",\", *df.columns).alias(\"value\"))\n",
    "df_text.write.mode(\"overwrite\").text(\"dbfs:/tmp/my_table_text\")\n",
    "dbutils.fs.ls(\"dbfs:/tmp/my_table_text\")\n",
    "df_readback = spark.read.text(\"dbfs:/tmp/my_table_text\").rdd\n",
    "'''\n",
    "\n",
    "print(type(df))\n",
    "print(df)\n",
    "\n",
    "df = df.rdd.map(lambda row: row['_c0'])\n",
    "print(df.collect()) # tom 70\n",
    "df = df.map(lambda line: line.split(\" \")[1])\n",
    "print(df.collect()) # 70. collect() returns 'list' type\n",
    "df_cnt = df.countByValue()\n",
    "print(df_cnt) # countByValue has made a 'dictionary type'\n",
    "\n",
    "\n",
    "\n",
    "for grade, count in sorted(df_cnt.items(), key=lambda item: item[1], reverse=True):\n",
    "# grade, count are the names of each column\n",
    "# items() function is going to return a list of a dictionary 'key-value' pairs\n",
    "# and is also going throught the loop with both keys and values\n",
    "    print(f\"{grade}: {count}\")\n",
    "    \n",
    "\n",
    "# grade_cnt = grade.countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0731abb3-e469-43ad-9090-98dce3514f50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[53]: [('a', 2), ('b', 1)]"
     ]
    }
   ],
   "source": [
    "sc = pyspark.SparkContext.getOrCreate();\n",
    "# Key / Value RDD\n",
    "\n",
    "# creating Key / Value RDD\n",
    "# total_by_brand = rdd.map(lambda brand: (brand, 1))\n",
    "\n",
    "# reduceByKey(): Merge the values for each key using an associative and commutative reduce function.\n",
    "from operator import add\n",
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "sorted(rdd.reduceByKey(add).collect())\n",
    "# [('a', 2), ('b', 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61417fe5-de97-4e6a-a7fb-2f9e749eed58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+---+\n|    _c0|  _c1|_c2|\n+-------+-----+---+\n|  seoul|10000|  3|\n|  seoul|10000|  5|\n|  seoul|40000|  7|\n|  busan| 5000|  7|\n|incheon| 4000|  2|\n|  busan| 9000|  4|\n|incheon| 5000|  7|\n|  seoul| 4000|  2|\n|  busan| 8000|  9|\n+-------+-----+---+\n\n+-------+-----+-----+\n|   city|price|count|\n+-------+-----+-----+\n|  seoul|10000|    3|\n|  seoul|10000|    5|\n|  seoul|40000|    7|\n|  busan| 5000|    7|\n|incheon| 4000|    2|\n|  busan| 9000|    4|\n|incheon| 5000|    7|\n|  seoul| 4000|    2|\n|  busan| 8000|    9|\n+-------+-----+-----+\n\n[Row(price=10000, count=3), Row(price=10000, count=5), Row(price=40000, count=7), Row(price=5000, count=7), Row(price=4000, count=2), Row(price=9000, count=4), Row(price=5000, count=7), Row(price=4000, count=2), Row(price=8000, count=9)]\n------------------\n[('10000', '3'), ('10000', '5'), ('40000', '7'), ('5000', '7'), ('4000', '2'), ('9000', '4'), ('5000', '7'), ('4000', '2'), ('8000', '9')]\n\n\n\n\n\n\n\n[('10000', (8, 2)), ('40000', ('7', 1)), ('5000', (14, 2)), ('4000', (4, 2)), ('9000', ('4', 1)), ('8000', ('9', 1))]\n[('10000', 4.0), ('40000', 7.0), ('5000', 7.0), ('4000', 2.0), ('9000', 4.0), ('8000', 9.0)]\n\n\n\n\n\n\n\n<class 'pyspark.rdd.PipelinedRDD'>\n[('10000', (4.0, 1)), ('40000', (7.0, 1)), ('5000', (7.0, 1)), ('4000', (2.0, 1)), ('9000', (4.0, 1)), ('8000', (9.0, 1))]\nOut[100]: 'avg_by_count = sum_of_count.mapValues(lambda total_count: int(total_count[0]) / total_count[1])\\nresults = avg_by_count.collect()\\nprint(type(result))\\n'"
     ]
    }
   ],
   "source": [
    "# default.house_price_csv\n",
    "data = spark.table(\"default.house_price_csv\") # dataframe\n",
    "data.show()\n",
    "data = data.withColumnRenamed(\"_c0\", \"city\").withColumnRenamed(\"_c1\", \"price\").withColumnRenamed(\"_c2\", \"count\") # giving names to columns\n",
    "data.show() # still dataframe\n",
    "\n",
    "# have to manually import col because 'import pyspark' only imports the top level\n",
    "from pyspark.sql.functions import col, when, lit\n",
    "\n",
    "parsed_data = data.select(\n",
    "    # selecting wanted columns from a dataframe\n",
    "    #col(\"city\"),\n",
    "    col(\"price\").cast(\"int\"),\n",
    "    col(\"count\").cast(\"int\")\n",
    ")\n",
    "\n",
    "print(parsed_data.collect())\n",
    "# [Row(price=10000, count=3), Row(price=10000, count=5), Row(price=40000, count=7), Row(price=5000, count=7), Row(price=4000, count=2), Row(price=9000, count=4), Row(price=5000, count=7), Row(price=4000, count=2), Row(price=8000, count=9)]\n",
    "print(\"------------------\")\n",
    "\n",
    "rdd = data.select(\"price\", \"count\").rdd.map(lambda row: (row[\"price\"], row[\"count\"])) # making dataframe -> rdd format data\n",
    "# print(rdd.collect()) # list of tuples finally\n",
    "\n",
    "result = rdd.mapValues(lambda count: (count, 1)).reduceByKey(lambda a, b: (int(a[0]) + int(b[0]), int(a[1]) + int(b[1])))\n",
    "# reduceByKey is 'grouping' by key and calculae(values)\n",
    "\n",
    "print(\"\\n\\n\\n\\n\\n\\n\")\n",
    "print(result.collect())\n",
    "# [('10000', (8, 2)), ('40000', ('7', 1)), ('5000', (14, 2)), ('4000', (4, 2)), ('9000', ('4', 1)), ('8000', ('9', 1))]\n",
    "\n",
    "sum_of_count = result.mapValues(lambda total_count: int(total_count[0]) / total_count[1])\n",
    "\n",
    "print(sum_of_count.collect())\n",
    "# [('10000', 4.0), ('40000', 7.0), ('5000', 7.0), ('4000', 2.0), ('9000', 4.0), ('8000', 9.0)]\n",
    "\n",
    "print(\"\\n\\n\\n\\n\\n\\n\")\n",
    "# result_rdd = rdd.mapValues(lambda count: count())\n",
    "\n",
    "\n",
    "avg_by_count = sum_of_count.mapValues(lambda count: (count, 1)).reduceByKey(lambda a, b: (int(a[0]) + int(b[0]), int(a[1]) + int(b[1])))\n",
    "\n",
    "print(avg_by_count.collect())\n",
    "# [('10000', (4.0, 1)), ('40000', (7.0, 1)), ('5000', (7.0, 1)), ('4000', (2.0, 1)), ('9000', (4.0, 1)), ('8000', (9.0, 1))]\n",
    "\n",
    "\n",
    "'''avg_by_count = sum_of_count.mapValues(lambda total_count: int(total_count[0]) / total_count[1])\n",
    "results = avg_by_count.collect()\n",
    "print(type(result))\n",
    "'''\n",
    "# print(price_cnt.collect())\n",
    "\n",
    "# to print RDD\n",
    "# print(price_cnt.first())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbb306f4-3d6e-4df1-a9cb-216baf1cd800",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+---+----+----------------------+---------------------------------+--------+----------+-----------+--------+---------+\n|record_id|month|day|year|AverageTemperatureFahr|AverageTemperatureUncertaintyFahr|    City|country_id|    Country|Latitude|Longitude|\n+---------+-----+---+----+----------------------+---------------------------------+--------+----------+-----------+--------+---------+\n|   474376|   01| 01|1853|                    NA|                               NA|Auckland|       NEW|New Zealand|  36.17S|  175.03E|\n|   474377|   02| 01|1853|                    NA|                               NA|Auckland|       NEW|New Zealand|  36.17S|  175.03E|\n|   474378|   03| 01|1853|                    NA|                               NA|Auckland|       NEW|New Zealand|  36.17S|  175.03E|\n|   474379|   04| 01|1853|                    NA|                               NA|Auckland|       NEW|New Zealand|  36.17S|  175.03E|\n|   474380|   05| 01|1853|                    NA|                               NA|Auckland|       NEW|New Zealand|  36.17S|  175.03E|\n|   474381|   06| 01|1853|               51.9062|                          36.9572|Auckland|       NEW|New Zealand|  36.17S|  175.03E|\n|   474382|   07| 01|1853|               52.3886|                          34.5488|Auckland|       NEW|New Zealand|  36.17S|  175.03E|\n|   474383|   08| 01|1853|                52.853|                          33.5498|Auckland|       NEW|New Zealand|  36.17S|  175.03E|\n|   474384|   09| 01|1853|               52.5776|                           33.638|Auckland|       NEW|New Zealand|  36.17S|  175.03E|\n|   474385|   10| 01|1853|               54.8726|                          33.9836|Auckland|       NEW|New Zealand|  36.17S|  175.03E|\n|   474386|   11| 01|1853|               56.6888|                          34.2518|Auckland|       NEW|New Zealand|  36.17S|  175.03E|\n|   474387|   12| 01|1853|                59.846|                          37.5062|Auckland|       NEW|New Zealand|  36.17S|  175.03E|\n|   474388|   01| 01|1854|               64.5908|                            36.23|Auckland|       NEW|New Zealand|  36.17S|  175.03E|\n|   474389|   02| 01|1854|                65.372|                          35.6576|Auckland|       NEW|New Zealand|  36.17S|  175.03E|\n|   474390|   03| 01|1854|               64.9688|                          35.3966|Auckland|       NEW|New Zealand|  36.17S|  175.03E|\n|   474391|   04| 01|1854|                59.927|                          35.2022|Auckland|       NEW|New Zealand|  36.17S|  175.03E|\n|   474392|   05| 01|1854|               57.5042|                           34.511|Auckland|       NEW|New Zealand|  36.17S|  175.03E|\n|   474393|   06| 01|1854|                54.518|                          34.7972|Auckland|       NEW|New Zealand|  36.17S|  175.03E|\n|   474394|   07| 01|1854|               53.1914|                           35.024|Auckland|       NEW|New Zealand|  36.17S|  175.03E|\n|   474395|   08| 01|1854|               52.8044|                          34.3544|Auckland|       NEW|New Zealand|  36.17S|  175.03E|\n+---------+-----+---+----+----------------------+---------------------------------+--------+----------+-----------+--------+---------+\nonly showing top 20 rows\n\n[('Auckland', 49.856), ('NA', 12.4682), ('Brasília', 62.9744), ('Canoas', 50.009), ('Cape Town', 49.9946), ('Hamilton', 44.564), ('Johannesburg', 42.1772), ('Kherson', 7.0952), ('Kiev', 2.85619999999999), ('Lvov', 7.1726), ('Marseille', 39.3908), ('Odesa', 14.8838), ('Paris', 25.0232), ('Stockholm', 13.3988), ('Tokyo', 29.156), ('Tottori', 34.2518), ('Uppsala', 6.0494), ('Warsaw', 6.8), ('Wroclaw', 9.167)]\nAuckland: 49.856\nBrasília: 62.9744\nCanoas: 50.009\nCape Town: 49.9946\nHamilton: 44.564\nJohannesburg: 42.1772\nKherson: 7.0952\nKiev: 2.85619999999999\nLvov: 7.1726\nMarseille: 39.3908\nOdesa: 14.8838\nParis: 25.0232\nStockholm: 13.3988\nTokyo: 29.156\nTottori: 34.2518\nUppsala: 6.0494\nWarsaw: 6.8\nWroclaw: 9.167\n"
     ]
    }
   ],
   "source": [
    " # practice 3\n",
    "\n",
    "df = spark.table(\"default.temperature_1_csv\")\n",
    "df.show()\n",
    "\n",
    "\n",
    "df = df.withColumnRenamed(\"City\", \"city\").withColumnRenamed(\"AverageTemperatureFahr\", \"avg_temp_fahr\")\n",
    "\n",
    "parsed_data = df.select(\n",
    "    # selecting wanted columns from a dataframe\n",
    "    #col(\"city\"),\n",
    "    col(\"city\"),\n",
    "    col(\"avg_temp_fahr\").cast(\"int\")\n",
    ")\n",
    "\n",
    "# changing the dataframe into rdd\n",
    "rdd = df.select(\"city\", \"avg_temp_fahr\")\n",
    "rdd = df.select(\"city\", \"avg_temp_fahr\").rdd.map(lambda row: (row[\"city\"], row[\"avg_temp_fahr\"]))\n",
    "# print(rdd.collect())\n",
    "# filter \"NA\" value\n",
    "rdd = rdd.filter(lambda x: \"NA\" not in x[1])\n",
    "\n",
    "# min temperature\n",
    "min_temp = rdd.reduceByKey(lambda x, y: min(float(x), float(y))) # comparing multiple values with one specific key\n",
    "final_list = min_temp.collect()\n",
    "print(final_list)\n",
    "\n",
    "filtered_final_list = [(city, float(temperature)) for city, temperature in final_list if city!= 'NA']\n",
    "\n",
    "for city, temperature in filtered_final_list:\n",
    "    print(f\"{city}: {temperature}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9efb83c3-f7dd-4809-90cd-3f85658a6ca6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- _c0: string (nullable = true)\n |-- _c1: string (nullable = true)\n |-- _c2: string (nullable = true)\n |-- _c3: string (nullable = true)\n |-- _c4: string (nullable = true)\n\n6: the\n4: Lorem\n4: of\n3: Ipsum\n3: and\n2: dummy\n2: text\n2: typesetting\n2: has\n2: a\n2: type\n2: It\n2: with\n1: is\n1: simply\n1: printing\n1: industry.\n1: been\n1: industry's\n1: standard\n1: ever\n1: since\n1: 1500s\n1: when\n1: an\n1: unknown\n1: printer\n1: took\n1: galley\n1: scrambled\n1: it\n1: to\n1: make\n1: specimen\n1: book.\n1: survived\n1: not\n1: only\n1: five\n1: centuries\n1: but\n1: also\n1: leap\n1: into\n1: electronic\n1: remaining\n1: essentially\n1: unchanged.\n1: was\n1: popularised\n1: in\n1: 1960s\n1: release\n1: Letraset\n1: sheets\n1: containing\n1: passages\n1: more\n1: recently\n1: desktop\n1: publishing\n1: software\n1: like\n1: Aldus\n1: PageMaker\n1: including\n1: versions\n1: Ipsum.\n"
     ]
    }
   ],
   "source": [
    "# practice 4. flatMap\n",
    "df = spark.table(\"default.lorem_ipsum_1_txt\")\n",
    "df.printSchema()\n",
    "combined_rows = df.rdd.map(lambda row: \" \".join([str(c) for c in row if c is not None]))\n",
    "\n",
    "words_rdd = combined_rows.flatMap(lambda line: line.split())\n",
    "# print(words_rdd.collect())\n",
    "\n",
    "# count\n",
    "'''word_cnt = words_rdd.countByValue() # create a dictionary\n",
    "for word, count in word_cnt.items(): # items() is used with dictionaries to get a view of the dictionary's key-value pairs\n",
    "    print(f\"{word}: {count}\")\n",
    "'''\n",
    "\n",
    "# sorting by the most frequent word\n",
    "word_cnt = words_rdd.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y) # calculation on the 'values' grouping by the 'key'\n",
    "# print(word_cnt.collect())\n",
    "sorted_word_count = word_cnt.map(lambda x: (x[1], x[0])).sortByKey(ascending=False)\n",
    "#print(sorted_word_count.collect())\n",
    "\n",
    "for word, count in sorted_word_count.collect():\n",
    "    print(f\"{word}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4b80926-01d2-4cf9-95ef-87502eaa7de2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+--------------------+------------+\n|              name|             country|               email|compensation|\n+------------------+--------------------+--------------------+------------+\n|  Willian Cummings|             Senegal|    areus@test.canon|       77369|\n|      Clarita Gill|             Ecuador| tomaslau@test.games|       86986|\n| Walter Washington|          Kazakhstan|mbilderbach@examp...|       91072|\n|       Lexie Banks|                Mali|unterdreht@test.date|       97933|\n|        Luise Hunt|               Kenya|adellecharles@tes...|       96175|\n|     Sebrina Walsh|         Puerto Rico|andrewcohen@examp...|       99276|\n|      Josiah Lyons|              Malawi|nandini_m@test.ry...|       91768|\n|      Temeka Grant|              Israel|terryxlife@test.g...|       71642|\n|  Narcisa Saunders|Palestinian Terri...|raquelwilson@exam...|       77287|\n|      Lisbeth Lane|          Azerbaijan|coreyweb@test.coffee|       82473|\n|       Evan Lawson|               Tonga|claudioguglieri@e...|       84796|\n|Gearldine Mcdaniel|            Slovenia|xtopherpaul@examp...|       96005|\n|   Kristel Jenkins|        Cook Islands|randomlies@exampl...|       79421|\n|   Douglass Porter|            Cambodia|ludwiczakpawel@ex...|       79263|\n|      Ahmed Warren|              Israel|brajeshwar@exampl...|       90636|\n|       Norah Reyes|   Brunei Darussalam|   soffes@example.mt|       80723|\n|      Emerita Ward|       Guinea-Bissau|nandini_m@example...|       88913|\n|   Hassan Mcdaniel|            Kiribati|  soffes@example.day|       93460|\n|   Joannie Bradley|             Bolivia|ehsandiary@test.o...|       96565|\n|     Gaylene Ellis|Palestinian Terri...|timmillwood@examp...|       95569|\n+------------------+--------------------+--------------------+------------+\nonly showing top 20 rows\n\n+--------------------+-----+\n|             country|count|\n+--------------------+-----+\n|Palestinian Terri...|    5|\n|             Ecuador|    4|\n|          Madagascar|    3|\n|   Brunei Darussalam|    3|\n|             Andorra|    3|\n|               Niger|    3|\n|             Tunisia|    3|\n|              Zambia|    3|\n|    Pitcairn Islands|    3|\n|             Hungary|    3|\n|                Mali|    3|\n|Sao Tome and Prin...|    2|\n|               Italy|    2|\n|       Faroe Islands|    2|\n|Netherlands Antilles|    2|\n|Northern Mariana ...|    2|\n|Saint Pierre and ...|    2|\n|      American Samoa|    2|\n|              Jersey|    2|\n|        Cook Islands|    2|\n+--------------------+-----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# practice 5. DataFrame\n",
    "from pyspark.sql import (\n",
    "    Row, SparkSession\n",
    ")\n",
    "from pyspark.sql.functions import col, asc, desc\n",
    "\n",
    "df = spark.table(\"default.income_2_txt\")\n",
    "# print(df.show())\n",
    "\n",
    "df.createOrReplaceGlobalTempView(\"income\") # making the dataframe into the global temp view table\n",
    "spark = SparkSession.builder.appName(\"SparkSQL\").getOrCreate()\n",
    "\n",
    "# 1. Spark SQL\n",
    "df = spark.sql(\n",
    "    \"SELECT * FROM global_temp.income WHERE compensation >= 70000 AND compensation <= 100000\")\n",
    "    # when accessing the 'income' table, it is in the global_temp database, not the 'default' database\n",
    "df.show()\n",
    "\n",
    "# 2. DataFrame function\n",
    "df.groupBy(\"country\").count().orderBy(col(\"count\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea16ecd4-84e4-4812-a866-5123ad790fe4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---+--------------------+\n|           name|age|             country|\n+---------------+---+--------------------+\n| Lauryn Hubbard| 80|           Mauritius|\n|  Verena Dennis| 80|United States of ...|\n|  Janee Holland| 80|         Switzerland|\n|     Dung Logan| 80|             Bahamas|\n|  Jadwiga Ellis| 80|            Mongolia|\n|   Josiah Evans| 80|               Niger|\n|     Janay Ryan| 80|            Dominica|\n|   Katia Santos| 80|             Algeria|\n| Hilario Little| 80|           Singapore|\n|    Daron Allen| 80|Falkland Islands ...|\n| Patricia Mccoy| 80|            Pakistan|\n| Sherika Jordan| 80|             Mayotte|\n|Roselle Gardner| 80|             Senegal|\n|  Shavonne Hart| 80|       Cote d'Ivoire|\n|Svetlana Hanson| 80|             Albania|\n|  Lindsay Myers| 80|       Cote d'Ivoire|\n|    Samual Shaw| 79|              Tuvalu|\n|    Jewell Boyd| 79|      Cayman Islands|\n|    Arletha Ray| 79|                Mali|\n|  Arlie Rodgers| 79| Trinidad and Tobago|\n+---------------+---+--------------------+\nonly showing top 20 rows\n\n+--------------------+-------+\n|             country|avg_age|\n+--------------------+-------+\n|                Chad|  36.25|\n|            Paraguay|  47.78|\n|            Anguilla|   72.0|\n|               Macao|   72.0|\n|Heard Island and ...|   30.0|\n|             Senegal|   53.0|\n|              Sweden|  45.33|\n|             Tokelau|  34.17|\n|French Southern T...|  50.67|\n|            Kiribati|  48.67|\n|   Republic of Korea|  58.17|\n|              Guyana|   39.0|\n|             Eritrea|  39.75|\n|              Jersey|   58.8|\n|         Philippines|  48.33|\n|            Djibouti|   38.6|\n|               Tonga|   49.0|\n|      Norfolk Island|  35.33|\n|            Malaysia|  60.67|\n|           Singapore|   40.0|\n+--------------------+-------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    avg, col, round as rnd\n",
    ")\n",
    "\n",
    "# spark = SparkSession.builder.appName(\"sql_import_csv\").getOrCreate()\n",
    "\n",
    "df = spark.table(\"age_csv\")\n",
    "# typecasting the 'age' column into 'int' type\n",
    "df = df.withColumn(\"age\", col(\"age\").cast(\"int\"))\n",
    "\n",
    "df.filter(df.age > 40).orderBy(col(\"age\").desc()).show()\n",
    "# orderBy(col(\"count\").desc()\n",
    "\n",
    "# df.groupBy(\"age\").count().show()\n",
    "# df.select(df.name, df.age, df.age - 10).show()\n",
    "\n",
    "\n",
    "df.select(df.name, df.age, df.country).groupBy(\"country\").agg(rnd(avg(\"age\"),2).alias(\"avg_age\")).show()#.sort(\"avg(age)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9000dc61-6a12-425b-80c4-6ad48d0b1c97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "lecture 2",
   "widgets": {
    "dummy": {
     "currentValue": "",
     "nuid": "da1205e2-f295-4016-8921-e9b497aa899a",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "",
      "name": "dummy",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "",
      "name": "dummy",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "file_upload": {
     "currentValue": "",
     "nuid": "ddfb495c-c816-4776-8da8-a9d8d529f321",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "",
      "name": "file_upload",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "",
      "name": "file_upload",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}